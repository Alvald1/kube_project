- name: Create keepalived.conf on control_panel nodes
  ansible.builtin.copy:
    dest: /etc/keepalived/keepalived.conf
    content: |
      global_defs {
          enable_script_security
          script_user nobody
      }

      vrrp_script check_apiserver {
        script "/etc/keepalived/check_apiserver.sh"
        interval 3
      }

      vrrp_instance VI_1 {
          state BACKUP
          interface ens33
          virtual_router_id 5
          priority 100
          advert_int 1
          nopreempt
          authentication {
              auth_type PASS
              auth_pass ZqSj#f1G
          }
          virtual_ipaddress {
              192.168.62.200
          }
          track_script {
              check_apiserver
          }
      }
    owner: root
    group: root
    mode: '0644'
  when: "'control_panel' in group_names"

- name: Create check_apiserver.sh on control_panel nodes
  ansible.builtin.copy:
    dest: /etc/keepalived/check_apiserver.sh
    content: |
      #!/bin/sh
      # File: /etc/keepalived/check_apiserver.sh

      APISERVER_VIP=192.168.62.200
      APISERVER_DEST_PORT=8888
      PROTO=http

      errorExit() {
          echo "*** $*" 1>&2
          exit 1
      }

      curl --silent --max-time 2 --insecure ${PROTO}://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET ${PROTO}://localhost:${APISERVER_DEST_PORT}/"
      if ip addr | grep -q ${APISERVER_VIP}; then
          curl --silent --max-time 2 --insecure ${PROTO}://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET ${PROTO}://${APISERVER_VIP}:${APISERVER_DEST_PORT}/"
      fi
    owner: root
    group: root
    mode: '0755'
  when: "'control_panel' in group_names"

- name: Ensure check_apiserver.sh is executable
  ansible.builtin.file:
    path: /etc/keepalived/check_apiserver.sh
    mode: '0755'
  when: "'control_panel' in group_names"

- name: Create haproxy.cfg on control_panel nodes
  ansible.builtin.copy:
    dest: /etc/haproxy/haproxy.cfg
    content: |
      # File: /etc/haproxy/haproxy.cfg
      #---------------------------------------------------------------------
      # Global settings
      #---------------------------------------------------------------------
      global
          log /dev/log local0 info alert
          log /dev/log local1 notice alert
          daemon

      #---------------------------------------------------------------------
      # common defaults that all the 'listen' and 'backend' sections will
      # use if not designated in their block
      #---------------------------------------------------------------------
      defaults
          mode                    http
          log                     global
          option                  httplog
          option                  dontlognull
          option http-server-close
          option forwardfor       except 127.0.0.0/8
          option                  redispatch
          retries                 1
          timeout http-request    10s
          timeout queue           20s
          timeout connect         5s
          timeout client          20s
          timeout server          20s
          timeout http-keep-alive 10s
          timeout check           10s

      #---------------------------------------------------------------------
      # apiserver frontend which proxys to the control plane nodes
      #---------------------------------------------------------------------
      frontend apiserver
          bind *:8888
          mode tcp
          option tcplog
          default_backend apiserver

      #---------------------------------------------------------------------
      # round robin balancing for apiserver
      #---------------------------------------------------------------------
      backend apiserver
          option httpchk GET /healthz
          http-check expect status 200
          mode tcp
          option ssl-hello-chk
          balance     roundrobin
              server node1 192.168.62.133:6443 check
              server node2 192.168.62.134:6443 check
              server node3 192.168.62.135:6443 check
    owner: root
    group: root
    mode: '0644'
  when: "'control_panel' in group_names"

- name: Enable keepalived service
  ansible.builtin.systemd:
    name: keepalived
    enabled: yes
  when: "'control_panel' in group_names"

- name: Start keepalived service
  ansible.builtin.systemd:
    name: keepalived
    state: started
  when: "'control_panel' in group_names"

- name: Enable haproxy service
  ansible.builtin.systemd:
    name: haproxy
    enabled: yes
  when: "'control_panel' in group_names"

- name: Restart haproxy service
  ansible.builtin.systemd:
    name: haproxy
    state: restarted
  when: "'control_panel' in group_names"

- name: Initialize Kubernetes control plane on node1 and save output
  ansible.builtin.command: |
    kubeadm init \
      --cri-socket unix:///var/run/cri-dockerd.sock \
      --pod-network-cidr=10.244.0.0/16 \
      --control-plane-endpoint "192.168.62.200:8888" \
      --upload-certs
  args:
    creates: /etc/kubernetes/admin.conf  
  when: inventory_hostname == 'node1'

- name: Upload certs and get certificate key (on node1)
  ansible.builtin.command: kubeadm init phase upload-certs --upload-certs
  register: upload_certs_result
  when: inventory_hostname == 'node1'
  run_once: true

- name: Extract certificate key from upload-certs output (on node1)
  ansible.builtin.set_fact:
    kubeadm_certificate_key: "{{ upload_certs_result.stdout | regex_findall('([a-f0-9]{64})') | first }}"
  when: inventory_hostname == 'node1'
  run_once: true

- name: Generate kubeadm join command for control-plane (on node1)
  ansible.builtin.command: kubeadm token create --print-join-command
  register: kubeadm_join_cmd_result
  when: inventory_hostname == 'node1'
  run_once: true

- name: Compose full workers join command (on node1)
  ansible.builtin.set_fact:
    kubeadm_join_command_workers: '{{ kubeadm_join_cmd_result.stdout }}'
  when: inventory_hostname == 'node1'
  run_once: true

- name: Compose full control-plane join command (on node1)
  ansible.builtin.set_fact:
    kubeadm_join_command_control: "{{ kubeadm_join_command_workers }} --control-plane --certificate-key {{ kubeadm_certificate_key }}"
  when: inventory_hostname == 'node1'
  run_once: true

- name: Set join command fact on all hosts
  ansible.builtin.set_fact:
    kubeadm_join_command_control: "{{ hostvars['node1']['kubeadm_join_command_control'] }}"
  when: inventory_hostname != 'node1'

- name: Set join command fact on all hosts
  ansible.builtin.set_fact:
    kubeadm_join_command_workers: "{{ hostvars['node1']['kubeadm_join_command_workers'] }}"
  when: inventory_hostname != 'node1'

- name: Join control-plane nodes to the Kubernetes cluster
  ansible.builtin.shell: "{{ kubeadm_join_command_control }} --cri-socket unix:///var/run/cri-dockerd.sock --ignore-preflight-errors=all"
  args:
    creates: /etc/kubernetes/admin.conf
  when:
    - "'control_panel' in group_names"
    - inventory_hostname != 'node1'

- name: Check if worker node already joined the cluster
  ansible.builtin.command: kubectl get node {{ inventory_hostname }}.internal -o name --kubeconfig /etc/kubernetes/admin.conf
  register: kube_worker_exists
  delegate_to: node1
  ignore_errors: yes
  changed_when: false
  failed_when: false
  when:
    - "'workers' in group_names"

- name: Join workers nodes to the Kubernetes cluster
  ansible.builtin.shell: "{{ kubeadm_join_command_workers }} --cri-socket unix:///var/run/cri-dockerd.sock --ignore-preflight-errors=all"
  args:
    creates: /etc/kubernetes/admin.conf
  when:
    - "'workers' in group_names"
    - kube_worker_exists is defined
    - kube_worker_exists.rc != 0

- name: Ensure .kube directory exists
  become: yes
  ansible.builtin.file:
    path: /home/{{ ansible_user | default('user') }}/.kube
    state: directory
    owner: "{{ ansible_user | default('user') }}"
    group: "{{ ansible_user | default('user') }}"
    mode: '0700'
  when: "'control_panel' in group_names"

- name: Copy kubeconfig to user home
  become: yes
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: /home/{{ ansible_user | default('user') }}/.kube/config
    owner: "{{ ansible_user | default('user') }}"
    group: "{{ ansible_user | default('user') }}"
    mode: '0600'
    remote_src: yes
  when: "'control_panel' in group_names"

- name: Set KUBECONFIG in /etc/environment
  become: yes
  ansible.builtin.lineinfile:
    path: /etc/environment
    line: KUBECONFIG=/home/{{ ansible_user | default('user') }}/.kube/config
    create: yes
  when: "'control_panel' in group_names"

- name: Export KUBECONFIG for current session
  ansible.builtin.shell: export KUBECONFIG=/home/{{ ansible_user | default('user') }}/.kube/config
  environment:
    KUBECONFIG: /home/{{ ansible_user | default('user') }}/.kube/config
  changed_when: false
  when: "'control_panel' in group_names"

- name: Apply Flannel CNI plugin (only on node1)
  become: yes
  ansible.builtin.command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  when: inventory_hostname == 'node1'