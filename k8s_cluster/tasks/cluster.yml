# ---------------------------
# 1. Настройка keepalived и haproxy (до инициализации кластера)
# ---------------------------
- name: Create keepalived.conf on control_panel nodes
  ansible.builtin.template:
    src: templates/keepalived.conf.j2
    dest: /etc/keepalived/keepalived.conf
    owner: root
    group: root
    mode: '0644'
  when: "'control_panel' in group_names"

- name: Create check_apiserver.sh on control_panel nodes
  ansible.builtin.template:
    src: templates/check_apiserver.sh.j2
    dest: /etc/keepalived/check_apiserver.sh
    owner: root
    group: root
    mode: '0755'
  when: "'control_panel' in group_names"

- name: Ensure check_apiserver.sh is executable
  ansible.builtin.file:
    path: /etc/keepalived/check_apiserver.sh
    mode: '0755'
  when: "'control_panel' in group_names"

- name: Create haproxy.cfg on control_panel nodes
  ansible.builtin.template:
    src: templates/haproxy1.cfg.j2
    dest: /etc/haproxy/haproxy.cfg
    owner: root
    group: root
    mode: '0644'
  when: "'control_panel' in group_names"

- name: Enable keepalived service
  ansible.builtin.systemd:
    name: keepalived
    enabled: yes
  when: "'control_panel' in group_names"

- name: Start keepalived service
  ansible.builtin.systemd:
    name: keepalived
    state: started
  when: "'control_panel' in group_names"

- name: Enable haproxy service
  ansible.builtin.systemd:
    name: haproxy
    enabled: yes
  when: "'control_panel' in group_names"

- name: Restart haproxy service
  ansible.builtin.systemd:
    name: haproxy
    state: restarted
  when: "'control_panel' in group_names"

# ---------------------------
# 2. Инициализация control-plane на node1
# ---------------------------
- name: Initialize Kubernetes control plane on node1 and save output
  ansible.builtin.command: |
    kubeadm init \
      --cri-socket unix:///var/run/cri-dockerd.sock \
      --pod-network-cidr={{ pod_network }} \
      --control-plane-endpoint "{{ network_prefix }}{{ ip_base_virtual }}:{{ api_port }}" \
      --upload-certs
  args:
    creates: /etc/kubernetes/admin.conf  
  when: inventory_hostname == 'node1'

# ---------------------------
# 3. Генерация и распространение join-команд
# ---------------------------
- name: Upload certs and get certificate key (on node1)
  ansible.builtin.command: kubeadm init phase upload-certs --upload-certs
  register: upload_certs_result
  when: inventory_hostname == 'node1'
  run_once: true

- name: Extract certificate key from upload-certs output (on node1)
  ansible.builtin.set_fact:
    kubeadm_certificate_key: "{{ upload_certs_result.stdout | regex_findall('([a-f0-9]{64})') | first }}"
  when: inventory_hostname == 'node1'
  run_once: true

- name: Generate kubeadm join command for control-plane (on node1)
  ansible.builtin.command: kubeadm token create --print-join-command
  register: kubeadm_join_cmd_result
  when: inventory_hostname == 'node1'
  run_once: true

- name: Compose full workers join command (on node1)
  ansible.builtin.set_fact:
    kubeadm_join_command_workers: '{{ kubeadm_join_cmd_result.stdout }}'
  when: inventory_hostname == 'node1'
  run_once: true

- name: Compose full control-plane join command (on node1)
  ansible.builtin.set_fact:
    kubeadm_join_command_control: "{{ kubeadm_join_command_workers }} --control-plane --certificate-key {{ kubeadm_certificate_key }}"
  when: inventory_hostname == 'node1'
  run_once: true

- name: Set join command fact on all hosts
  ansible.builtin.set_fact:
    kubeadm_join_command_control: "{{ hostvars['node1']['kubeadm_join_command_control'] }}"
  when: inventory_hostname != 'node1'

- name: Set join command fact on all hosts
  ansible.builtin.set_fact:
    kubeadm_join_command_workers: "{{ hostvars['node1']['kubeadm_join_command_workers'] }}"
  when: inventory_hostname != 'node1'

# ---------------------------
# 4. Присоединение control-plane и worker-нод
# ---------------------------
- name: Join control-plane nodes to the Kubernetes cluster
  ansible.builtin.command: "{{ kubeadm_join_command_control }} --cri-socket unix:///var/run/cri-dockerd.sock --ignore-preflight-errors=all"
  args:
    creates: /etc/kubernetes/admin.conf
  when:
    - "'control_panel' in group_names"
    - inventory_hostname != 'node1'

- name: Check if worker node already joined the cluster
  ansible.builtin.command: kubectl get node {{ inventory_hostname }}.internal -o name
  register: kube_worker_exists
  delegate_to: node1
  ignore_errors: yes
  changed_when: false
  failed_when: false
  environment:
    KUBECONFIG: "{{ kube_pwd }}/config"
  when:
    - "'workers' in group_names"

- name: Join workers nodes to the Kubernetes cluster
  ansible.builtin.command: "{{ kubeadm_join_command_workers }} --cri-socket unix:///var/run/cri-dockerd.sock --ignore-preflight-errors=all"
  args:
    creates: /etc/kubernetes/admin.conf
  when:
    - "'workers' in group_names"
    - kube_worker_exists is defined
    - kube_worker_exists.rc != 0

# ---------------------------
# 5. Копирование kubeconfig и установка CNI
# ---------------------------
- name: Ensure .kube directory exists
  become: yes
  ansible.builtin.file:
    path: "{{ kube_pwd }}"
    state: directory
    owner: "{{ ansible_user | default('user') }}"
    group: "{{ ansible_user | default('user') }}"
    mode: '0700'
  when: "'control_panel' in group_names"

- name: Copy kubeconfig to user home
  become: yes
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ kube_pwd }}/config"
    owner: "{{ ansible_user | default('user') }}"
    group: "{{ ansible_user | default('user') }}"
    mode: '0600'
    remote_src: yes
  when: "'control_panel' in group_names"

- name: Apply Flannel CNI plugin (only on node1)
  become: yes
  ansible.builtin.command: kubectl apply -f {{ kube_flannel }}
  environment:
    KUBECONFIG: "{{ kube_pwd }}/config"
  when: inventory_hostname == 'node1'

# ---------------------------
# 6. Установка ingress-nginx (namespace и Helm)
# ---------------------------
- name: Create ingress-nginx namespace
  kubernetes.core.k8s:
    api_version: v1
    kind: Namespace
    name: ingress-nginx
    state: present
  delegate_to: node1
  environment:
    KUBECONFIG: "{{ kube_pwd }}/config"
  run_once: true

- name: Install ingress-nginx via Helm
  kubernetes.core.helm:
    name: ingress-nginx
    chart_ref: ingress-nginx/ingress-nginx
    release_namespace: ingress-nginx
    create_namespace: false
    state: present
  delegate_to: node1
  environment:
    KUBECONFIG: "{{ kube_pwd }}/config"
  run_once: true

# ---------------------------
# 7. Настройка haproxy (после получения ingress NodePort)
# ---------------------------
- name: Get ingress-nginx NodePort for HTTP
  ansible.builtin.command: |
    kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath="{.spec.ports[?(@.port==80)].nodePort}"
  environment:
    KUBECONFIG: "{{ kube_pwd }}/config"
  register: ingress_nodeport_result
  delegate_to: node1
  run_once: true
  changed_when: false

- name: Set ingress NodePort fact
  set_fact:
    ingress_nodeport: "{{ ingress_nodeport_result.stdout }}"
  when: ingress_nodeport_result.stdout is defined

- name: Create haproxy.cfg on control_panel nodes
  ansible.builtin.template:
    src: templates/haproxy2.cfg.j2
    dest: /etc/haproxy/haproxy.cfg
    owner: root
    group: root
    mode: '0644'
  when: "'control_panel' in group_names"

- name: Restart haproxy service
  ansible.builtin.systemd:
    name: haproxy
    state: restarted
  when: "'control_panel' in group_names"