# Ansible роль `k8s_cluster` для автоматизированного развертывания Kubernetes-кластера

## Описание роли

Роль `k8s_cluster` предназначена для полного автоматизированного развертывания Kubernetes-кластера на базе Debian 12 (bookworm) с поддержкой различных сценариев:

- **HA (High Availability) control-plane** с использованием keepalived и haproxy.
- **Выбор контейнерного движка**: CRI-O, containerd или Docker+cri-dockerd.
- **Гибкая настройка сети, репозиториев, зависимостей и CNI**.
- **Автоматическая установка ingress-nginx и деплой тестового приложения**.

Роль реализует все этапы подготовки и установки кластера, включая:

- Настройку сетевых параметров, hostname и hosts-файла.
- Добавление и настройку репозиториев Debian и Kubernetes.
- Установку всех необходимых зависимостей.
- Конфигурирование ядра и системных параметров для работы Kubernetes.
- Установку выбранного контейнерного рантайма.
- Инициализацию control-plane и присоединение worker-нод.
- Автоматическую установку CNI (Flannel).
- Развёртывание ingress-nginx через Helm.
- Настройку keepalived и haproxy для обеспечения отказоустойчивости API и приложений.

## Архитектура кластера

### Состав кластера

- **Control-plane (группа `[control_panel]`)**: Несколько узлов (например, node1, node2, node3), на которых развёрнуты компоненты управления Kubernetes (kube-apiserver, etcd, controller-manager, scheduler).
- **Worker-узлы (группа `[workers]`)**: Узлы для запуска пользовательских подов и сервисов.
- **Виртуальный IP (VIP)**: Для HA используется keepalived, который обеспечивает виртуальный IP-адрес для доступа к API Kubernetes.
- **HAProxy**: Балансирует трафик к API-серверам и ingress-контроллерам между control-plane и worker-нодами.

### Принцип работы

1. **Сетевые настройки**: На каждом узле настраиваются hostname и /etc/hosts для корректного разрешения имён внутри кластера.
2. **Репозитории и зависимости**: Добавляются необходимые репозитории Debian, Kubernetes, контейнерных движков. Устанавливаются все зависимости.
3. **Контейнерный рантайм**: В зависимости от переменной `branch_mode` устанавливается CRI-O, containerd или Docker+cri-dockerd.
4. **Kubernetes**: На первом control-plane (обычно node1) выполняется `kubeadm init` с параметрами для HA (VIP, загрузка сертификатов). Остальные control-plane и worker-узлы присоединяются автоматически с помощью сгенерированных join-команд.
5. **CNI**: После инициализации применяется Flannel для организации сетевого взаимодействия между подами.
6. **Ingress**: Устанавливается ingress-nginx через Helm для публикации сервисов наружу.
7. **HA и балансировка**:
   - **keepalived** обеспечивает виртуальный IP (VIP) для отказоустойчивого доступа к API Kubernetes.
   - **haproxy** балансирует трафик к API и ingress между control-plane и worker-нодами.
   - Скрипты проверки состояния API автоматически переключают VIP между control-plane при сбоях.
8. **Доступ к кластеру**: kubeconfig копируется в домашнюю директорию пользователя на control-plane для удобного доступа к kubectl.

### Сценарии развертывания

- **HA-кластер** (`branch_mode: 2`): Несколько control-plane, keepalived+haproxy, Docker+cri-dockerd.
- **Одновузловой кластер** (`branch_mode: 0` или `1`): Один control-plane, без HA, CRI-O или containerd.

## Использование

1. Настройте `inventory.ini` с нужными узлами.
2. Укажите параметры в `defaults/main.yml` или через переменные.
3. Запустите основной playbook:
   ```bash
   ansible-playbook -i inventory.ini playbook.yml
   ```
4. После завершения деплоя кластер готов к работе, ingress доступен через VIP.

## Преимущества архитектуры

- **Отказоустойчивость**: Благодаря keepalived и haproxy API и ingress остаются доступны при сбое любого control-plane.
- **Гибкость**: Лёгкая смена контейнерного движка и масштабирование кластера.
- **Автоматизация**: Весь процесс полностью автоматизирован Ansible, не требует ручных действий.
- **Безопасность**: Использование Ansible Vault для хранения чувствительных данных.

---
